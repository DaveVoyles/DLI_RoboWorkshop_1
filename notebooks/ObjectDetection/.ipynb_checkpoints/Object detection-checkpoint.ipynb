{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In the previous notebook we saw how you can classify images as specific objects.  This was good, but we saw how image classification can get confused when there are many objects in a camera view.\n",
    "\n",
    "In this notebook we will work through multiple examples of how to use DIGITS and Caffe to detect objects in imagery.  The data set we will be using is Common Objects in Context (CoCo).  This data set is provided by Microsoft and is a common benchmarking and academic data set.  It also provides a good baseline for robotics applications as there are many objects in individual images.  We will train a detection model with this dataset and deploy it to the TX-1 platform.\n",
    "\n",
    "Fig 1 shows an example image containing a horse crossing sign:\n",
    "\n",
    "![Horse Crossing](COCO_test2015_000000387637.jpg)\n",
    "<h4 align=\"center\">Figure 1: Horse Crossing Sign</h4> \n",
    "\n",
    "\n",
    "We are going to tackle a very interesting problem in this tutorial.  Rather than trying to identify the image as a single object, we are going to train a convolutional neural network (CNN) to localize various objects within the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection with DetectNet\n",
    "\n",
    "There is a final class of object approaches that train a CNN to simultaneously classify the most likely object present at each location within an image and predict the corresponding bounding box for that object through regression.  For example:\n",
    "\n",
    "![yolo](yolo.png)\n",
    "\n",
    "This approach has major benefits:\n",
    "\n",
    "* Simple one-shot detection, classification and bounding box regression pipeline\n",
    "* Very low latency\n",
    "* Very low false alarm rates due to strong, voluminous background training data\n",
    "\n",
    "In order to train this type of network specialized training data is required where all objects of interest are labelled with accurate bounding boxes.  This type of training data is much rarer and costly to produce; however, if this type of data is available for your object detection problem this is almost certainly the best approach to take. Fig 5 shows an example of a labelled training sample for a vehicle detection scenario.\n",
    "\n",
    "![kespry example](kespry_example.png)\n",
    "<h4 align=\"center\">Figure 6: Labelled data for a three class object detection scenario</h4> \n",
    "\n",
    "The recent release of DIGITS 4 added the capability to train this class of model and provided a new \"standard network\" called DetectNet as an example.  We are going to use DetectNet to train a Right Whale detector in full-size aerial images of the ocean.  \n",
    "\n",
    "The main challenge in training a single CNN for object detection and bounding box regression is in handling the fact that there can be varying numbers of objects present in different images.  In some cases you may even have an image with no objects at all.  DetectNet handles this problem by converting an image with an number of bounding box annotations to a fixed dimensionality data representation that we directly attempt to predict with a CNN.  Fig 6 shows how data is mapped to this represenation for a single class object detection problem.\n",
    "\n",
    "![detectnet data rep](detectnet_data.png)\n",
    "<h4 align=\"center\">Figure 7: DetectNet data representation</h4> \n",
    "\n",
    "DetectNet is actually a FCN, as we described above, but configured to produce precisely this data representation as it's output.  The bulk of the layers in DetectNet are identical to the well known GoogLeNet network.  Fig 7 shows the DetectNet architecture for training.\n",
    "\n",
    "![detectnet training architecture](detectnet_training.png)\n",
    "<h4 align=\"center\">Figure 8: DetectNet training architecture</h4> \n",
    "\n",
    "For the purposes of this lab we have already prepared the coco dataset for this specific use case within the digits ecosystem so we can begin training, however let us review how the label files work.  Digits uses a label file format known as \"Kitti\".  To begin with, we must first understand the folder structure.  Notice there is a 1 to 1 pairing of image to label file and both have exactly the same name except a different extension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Now we will first look at how to train DetectNet on this dataset. A complete training run of DetectNet on this dataset takes several hours, so we have provided a trained model to experiment with.  Return to the main DIGITS screen and use the Models tab.  Open the \"whale_detectnet\" model and clone it.  Make the following changes:\n",
    "\n",
    "* select your newly created \"coco_detectnet\" dataset\n",
    "* change the number of training epochs to 3  \n",
    "* change the batch size to 10\n",
    "\n",
    "Feel free to explore the network architecture visually by clicking the \"Visualize\" button.  \n",
    "\n",
    "When you're ready to train, give the model a new name like \"whale_detectnet_2\" and click \"create\".  Training this model for just 3 epochs will still take about 8 minutes, but you should see both the coverage and bounding box training and validation loss values decreasing already.  You will also see the mean Average Precision (mAP) score begin to rise.  mAP is a combined measure of how well the network is able to detect the whale faces and how accurate it's bounding box estimates were for the validation dataset.\n",
    "\n",
    "Once the model has finished training return to the pre-trained \"whale_detectnet\" model.  You will see that after 100 training epochs this model had not only converged to low training and validation loss values, but also a high mAP score.  Let's test this trained model against a validation image to see if it can find the whale face.\n",
    "\n",
    "Simply set the visualization method to \"Bounding boxes\" and paste the following image path in:  `/home/ubuntu/data/whale/data_336x224/val/images/000000118.png`.  Be sure to select the \"Show visualizations and statistics\" checkbox and then click \"Test One\".  You should see DetectNet successfully detects the whale face and draws a bounding box, like this:\n",
    "\n",
    "![detectnet success](detectnet_success.png)\n",
    "\n",
    "Feel free to test other images from the `/home/ubuntu/data/whale/data_336x224/val/images/` folder.  You will see that DetectNet is able to accurately detect most bottles with a tightly drawn bounding box and has a very low false alarm rate.  Furthermore, inference is extremely fast with DetectNet.  Execute the following cell to use the Caffe command line interface to carry out an inference benchmark using the DetectNet architecture.  You should see that the average time taken to pass a single 336x224 pixel image forward through DetectNet is just 22ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!caffe time --model /home/ubuntu/data/whale/deploy.prototxt --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to questions:\n",
    "\n",
    "<a id='answer1'></a>\n",
    "### Answer 1\n",
    "\n",
    "Random patches may end up containing whale faces too.  This is unlikely as the faces are typically only a very small part of the image and we have a large sample of random background patches which will almost entirely not contain whale faces.  We may also get whale bodies and tails in our background set, but this good as we are interested in localizing whale faces.\n",
    "\n",
    "[Click here](#question1) to return to question 1\n",
    "\n",
    "<a id='answer2'></a>\n",
    "### Answer 2\n",
    "\n",
    "Some good things to try would be to use a larger number of randomly selected non-face patches and to balance the dataset by augmenting the existing face patches with random rotations, flips and scalings.  You could also train a model with a larger number of trainable parameters such as GoogleNet.\n",
    "\n",
    "[Click here](#question2) to return to question 2\n",
    "\n",
    "<a id='answer3'></a>\n",
    "### Answer 3\n",
    "\n",
    "We could batch together multiple grid squares at a time to feed into the network for classification as a batch - that way we can further exploit parallelism and get computational acceleration from the GPU.\n",
    "\n",
    "[Click here](#question3) to return to question 3\n",
    "\n",
    "<a id='answer-optional-exercise'></a>\n",
    "\n",
    "### Answer to optional exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import time\n",
    "\n",
    "MODEL_JOB_NUM = '20160920-092148-8c17'  ## Remember to set this to be the job number for your model\n",
    "DATASET_JOB_NUM = '20160920-090913-a43d'  ## Remember to set this to be the job number for your dataset\n",
    "\n",
    "MODEL_FILE = '/home/ubuntu/digits/digits/jobs/' + MODEL_JOB_NUM + '/deploy.prototxt'                 # Do not change\n",
    "PRETRAINED = '/home/ubuntu/digits/digits/jobs/' + MODEL_JOB_NUM + '/snapshot_iter_270.caffemodel'    # Do not change\n",
    "MEAN_IMAGE = '/home/ubuntu/digits/digits/jobs/' + DATASET_JOB_NUM + '/mean.jpg'                      # Do not change\n",
    "\n",
    "# load the mean image\n",
    "mean_image = caffe.io.load_image(MEAN_IMAGE)\n",
    "\n",
    "# Choose a random image to test against\n",
    "RANDOM_IMAGE = str(np.random.randint(10))\n",
    "IMAGE_FILE = 'data/samples/w_' + RANDOM_IMAGE + '.jpg' \n",
    "\n",
    "# Tell Caffe to use the GPU\n",
    "caffe.set_mode_gpu()\n",
    "# Initialize the Caffe model using the model trained in DIGITS\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(256, 256))\n",
    "\n",
    "# Load the input image into a numpy array and display it\n",
    "input_image = caffe.io.load_image(IMAGE_FILE)\n",
    "plt.imshow(input_image)\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many 256x256 grid squares are in the image\n",
    "rows = input_image.shape[0]/256\n",
    "cols = input_image.shape[1]/256\n",
    "\n",
    "# Subtract the mean image\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        input_image[i*256:(i+1)*256,j*256:(j+1)*256] -= mean_image\n",
    "        \n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "        \n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square])\n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time (sliding window without overlap): ' + str(end-start) + ' seconds'\n",
    "\n",
    "# define the amount of overlap between grid cells\n",
    "OVERLAP = 0.25\n",
    "grid_rows = int((rows-1)/(1-OVERLAP))+1\n",
    "grid_cols = int((cols-1)/(1-OVERLAP))+1\n",
    "\n",
    "print \"Image has %d*%d blocks of 256 pixels\" % (rows, cols)\n",
    "print \"With overlap=%f grid_size=%d*%d\" % (OVERLAP, grid_rows, grid_cols)\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((grid_rows,grid_cols))\n",
    "\n",
    "# Iterate over each grid square using the model to make a class prediction\n",
    "start = time.time()\n",
    "for i in range(0,grid_rows):\n",
    "    for j in range(0,grid_cols):\n",
    "        start_col = j*256*(1-OVERLAP)\n",
    "        start_row = i*256*(1-OVERLAP)\n",
    "        grid_square = input_image[start_row:start_row+256, start_col:start_col+256]\n",
    "        # make prediction\n",
    "        prediction = net.predict([grid_square])\n",
    "        detections[i,j] = prediction[0].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print ('Total inference time (sliding window with %f%% overlap: ' % (OVERLAP*100)) + str(end-start) + ' seconds'\n",
    "\n",
    "# now with batched inference (one column at a time)\n",
    "# we are not using a caffe.Classifier here so we need to do the pre-processing\n",
    "# manually. The model was trained on random crops (256*256->227*227) so we\n",
    "# need to do the cropping below. Similarly, we need to convert images\n",
    "# from Numpy's Height*Width*Channel (HWC) format to Channel*Height*Width (CHW) \n",
    "# Lastly, we need to swap channels from RGB to BGR\n",
    "net = caffe.Net(MODEL_FILE, PRETRAINED, caffe.TEST)\n",
    "start = time.time()\n",
    "net.blobs['data'].reshape(*[grid_cols, 3, 227, 227])\n",
    "\n",
    "# Initialize an empty array for the detections\n",
    "detections = np.zeros((rows,cols))\n",
    "\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        grid_square = input_image[i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "        # add to batch\n",
    "        grid_square = grid_square[14:241,14:241] # 227*227 center crop        \n",
    "        image = np.copy(grid_square.transpose(2,0,1)) # transpose from HWC to CHW\n",
    "        image = image * 255 # rescale\n",
    "        image = image[(2,1,0), :, :] # swap channels\n",
    "        net.blobs['data'].data[j] = image\n",
    "    # make prediction\n",
    "    output = net.forward()[net.outputs[-1]]\n",
    "    for j in range(0,cols):\n",
    "        detections[i,j] = output[j].argmax()\n",
    "end = time.time()\n",
    "        \n",
    "# Display the predicted class for each grid square\n",
    "plt.imshow(detections)\n",
    "plt.show()\n",
    "\n",
    "# Display total time to perform inference\n",
    "print 'Total inference time (batched inference): ' + str(end-start) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Click here](#question-optional-exercise) to return to question\n",
    "\n",
    "<a id='answer4'></a>\n",
    "### Answer 4\n",
    "\n",
    "Replace layers fc6 through fc8 with the following. Then set the `bottom` blob of the `loss`, `accuracy` and `softmax` layers to `conv8`.\n",
    "\n",
    "```layer {\n",
    "  name: \"conv6\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool5\"\n",
    "  top: \"conv6\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 4096\n",
    "    pad: 0\n",
    "    kernel_size: 6\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu6\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv6\"\n",
    "  top: \"conv6\"\n",
    "}\n",
    "layer {\n",
    "  name: \"drop6\"\n",
    "  type: \"Dropout\"\n",
    "  bottom: \"conv6\"\n",
    "  top: \"conv6\"\n",
    "  dropout_param {\n",
    "    dropout_ratio: 0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv7\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv6\"\n",
    "  top: \"conv7\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 4096\n",
    "    kernel_size: 1\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"relu7\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv7\"\n",
    "}\n",
    "layer {\n",
    "  name: \"drop7\"\n",
    "  type: \"Dropout\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv7\"\n",
    "  dropout_param {\n",
    "    dropout_ratio: 0.5\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"conv8\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv7\"\n",
    "  top: \"conv8\"\n",
    "  param {\n",
    "    lr_mult: 1.0\n",
    "    decay_mult: 1.0\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2.0\n",
    "    decay_mult: 0.0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    kernel_size: 1\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "[Click here](#Exercise:) to return to the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Thanks and data attribution\n",
    "\n",
    "We would like to thank NOAA for their permission to re-use the Right Whale imagery for the purposes of this lab.\n",
    "\n",
    "Images were collected under MMPA Research permit numbers:\n",
    "\n",
    "MMPA 775-1600,      2005-2007\n",
    "\n",
    "MMPA 775-1875,      2008-2013 \n",
    "\n",
    "MMPA 17355,         2013-2018\n",
    "\n",
    "Photo Credits: NOAA/NEFSC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
